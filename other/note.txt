1.下載fb文章
2.轉簡體(因為之後用到的斷詞函式庫對於簡體的斷詞比較準)
3.斷詞  -->輸出到某個資料夾
4.stop word處理
5.決定vocabulary --> 文章中有出現的非stopwords
6.得到term-freq matrix  
7.輸出vocabulary和 term-freq matrix 到json裡面(可讀性)
8.實行LDA

問題:
做出來結果非常的奇怪...
而且還有一個問題 垃圾字很多
-->先把tokenized(加上除去Stopwords輸出到一個檔案...)
json.dump(l,f,ensure_ascii=False) -->沒有這一行輸出會變成literal string  "/u...." 幹

問題
就算沒有編碼問題弄出來結果還是很爛
因為沒有把動詞砍光光<--可能原因
為了驗證就去找dl4j的資料拿來train

問題
加入command line的模式方便訓練不一樣的資料...
但是中間產生的json檔(結果和term freq如果沒設置好都會被覆蓋掉)-->用git branch

問題
訓練dl4j的時候 最後的topic word一直會出現"/x00"(空字串)
但是完全不知道原因= =
